# final-project

ë¬¼ë¡ ì…ë‹ˆë‹¤! ê° íŒŒì´ì¬ ì½”ë“œ ë¸”ë¡ ë°‘ì— í•´ë‹¹ **ë‹¨ê³„ë³„ ì„¤ëª…**ì„ ë”°ë¡œ ì •ë¦¬í•´ì„œ ëª…í™•í•˜ê²Œ ì´í•´í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” **í…ìŠ¤íŠ¸ ìš”ì•½ ëª¨ë¸ ê°œë°œ ì „ì²´ ê³¼ì •**ì˜ ì½”ë“œì™€ ì„¤ëª…ì…ë‹ˆë‹¤.

---


âœ… ì‚¬ì „ ì¤€ë¹„
ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

bash
ë³µì‚¬
í¸ì§‘
pip install transformers datasets evaluate rouge-score

## ğŸ“ 1. `config.py`

```python
# config.py
MODEL_NAME = "facebook/bart-base"
DATASET_NAME = "cnn_dailymail"
DATASET_CONFIG = "3.0.0"
MAX_INPUT_LENGTH = 1024
MAX_TARGET_LENGTH = 128
BATCH_SIZE = 4
NUM_EPOCHS = 3
LEARNING_RATE = 5e-5
```

### ğŸ” ì„¤ëª…

* ì‚¬ìš©í•  ì‚¬ì „í•™ìŠµ ëª¨ë¸(BART)ì„ ì§€ì •í•©ë‹ˆë‹¤.
* CNN/Daily Mail ë°ì´í„°ì…‹ ë²„ì „ 3.0.0ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.
* ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ìš”ì•½ ê¸¸ì´, í•™ìŠµ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°(Batch Size, Epoch ìˆ˜ ë“±)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

---

## ğŸ§° 2. `utils.py`

```python
# utils.py
from transformers import AutoTokenizer

def preprocess_function(examples, tokenizer, max_input_len, max_target_len):
    inputs = examples["article"]
    targets = examples["highlights"]
    model_inputs = tokenizer(
        inputs, max_length=max_input_len, truncation=True, padding="max_length"
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=max_target_len, truncation=True, padding="max_length"
        )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

### ğŸ” ì„¤ëª…

* ê¸°ì‚¬ ë³¸ë¬¸(`article`)ê³¼ ìš”ì•½ë¬¸(`highlights`)ì„ í† í¬ë‚˜ì´ì§•í•©ë‹ˆë‹¤.
* ì…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ ì •í•´ì§„ ê¸¸ì´ë¡œ ìë¥´ê³  íŒ¨ë”©ì„ ì ìš©í•´ ëª¨ë¸ì´ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

---

## ğŸš€ 3. `main.py`

```python
# main.py
from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,
                          Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq)
import evaluate
from config import *
from utils import preprocess_function

dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

tokenized_datasets = dataset.map(
    lambda x: preprocess_function(x, tokenizer, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH),
    batched=True,
    remove_columns=dataset["train"].column_names,
)

model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    save_total_limit=2,
    predict_with_generate=True,
    logging_dir='./logs',
)

rouge = evaluate.load("rouge")
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    return rouge.compute(predictions=decoded_preds, references=decoded_labels)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

test_results = trainer.predict(tokenized_datasets["test"])
print("ROUGE ê²°ê³¼:", compute_metrics(test_results))
```

### ğŸ” ì„¤ëª…

1. **ë°ì´í„° ë¡œë“œ**: Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ CNN/DailyMail ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
2. **ì „ì²˜ë¦¬**: ê¸°ì‚¬ì™€ ìš”ì•½ì„ BART ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ í† í¬ë‚˜ì´ì§•í•©ë‹ˆë‹¤.
3. **ëª¨ë¸ ë¡œë“œ**: BART ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ fine-tuningì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
4. **í•™ìŠµ ì¸ì ì„¤ì •**: í•™ìŠµì— í•„ìš”í•œ ì„¤ì •(ë°°ì¹˜ ì‚¬ì´ì¦ˆ, í•™ìŠµë¥ , í‰ê°€ ì „ëµ ë“±)ì„ ì •ì˜í•©ë‹ˆë‹¤.
5. **í‰ê°€ì§€í‘œ ì„¤ì •**: ROUGE ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
6. **Trainer êµ¬ì„±**: Hugging Faceì˜ `Trainer` í´ë˜ìŠ¤ë¥¼ í™œìš©í•´ í•™ìŠµì„ êµ¬ì„±í•©ë‹ˆë‹¤.
7. **ëª¨ë¸ í•™ìŠµ**: fine-tuning ì‹œì‘.
8. **í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€**: í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ì…‹ì— ì ìš©í•´ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

---

## ğŸ§ª 4. ìš”ì•½ ê²°ê³¼ í™•ì¸ (í…ŒìŠ¤íŠ¸)

```python
article = dataset["test"][0]["article"]
ref = dataset["test"][0]["highlights"]

input_ids = tokenizer(article, return_tensors="pt", truncation=True).input_ids
summary_ids = model.generate(input_ids, max_length=MAX_TARGET_LENGTH)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("ğŸ“„ ì›ë¬¸:\n", article[:500])
print("\nğŸ“ ëª¨ë¸ ìš”ì•½:\n", summary)
print("\nâœ… ì°¸ì¡° ìš”ì•½:\n", ref)
```

### ğŸ” ì„¤ëª…

* í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ í•˜ë‚˜ì˜ ê¸°ì‚¬ì— ëŒ€í•´ ìš”ì•½ì„ ìƒì„±í•˜ê³ , ì‚¬ëŒì´ ì§ì ‘ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ì°¸ì¡° ìš”ì•½ê³¼ í•¨ê»˜ ì¶œë ¥í•©ë‹ˆë‹¤.

---

## âœ‚ï¸ 5. (ì„ íƒ) ì¶”ì¶œì  ìš”ì•½

```python
from sumy.summarizers.lsa import LsaSummarizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer

def extractive_summary(text, num_sentences=3):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, num_sentences)
    return " ".join(str(sentence) for sentence in summary)
```

### ğŸ” ì„¤ëª…

* Sumyì˜ LSA ë°©ì‹ìœ¼ë¡œ ê°„ë‹¨í•œ **ì¶”ì¶œì  ìš”ì•½**ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
* ì´ëŠ” ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ ë¬¸ì¥ì„ ì„ íƒí•´ ìš”ì•½ì„ ë§Œë“œëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ë³„ë„ì˜ í•™ìŠµ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## âœ… ì „ì²´ ìš”ì•½

| ë‹¨ê³„           | ì„¤ëª…                                  |
| ------------ | ----------------------------------- |
| 1. ì„¤ì • íŒŒì¼ ì‘ì„±  | ëª¨ë¸, ë°ì´í„°ì…‹, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •                |
| 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ êµ¬í˜„ | ê¸°ì‚¬ì™€ ìš”ì•½ì„ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ë³€í™˜                |
| 3. ë©”ì¸ ì½”ë“œ ì‹¤í–‰  | ë°ì´í„° ë¡œë“œ â†’ ëª¨ë¸ ì¤€ë¹„ â†’ í•™ìŠµ â†’ í‰ê°€ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ |
| 4. ê²°ê³¼ í™•ì¸     | ì‹¤ì œ ê¸°ì‚¬ì— ëŒ€í•´ ìƒì„±ëœ ìš”ì•½ì„ ìˆ˜ë™ìœ¼ë¡œ ë¹„êµ           |
| 5. ì¶”ê°€ ê¸°ë²•     | Sumy ë“±ìœ¼ë¡œ ì¶”ì¶œì  ìš”ì•½ë„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥             |

---

ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ìŒ ì‘ì—…ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤:

* âœ… Gradio UIë¡œ ì›¹ ë°ëª¨ êµ¬ì¶•
* âœ… Flask APIë¡œ ëª¨ë¸ ì„œë¹™
* âœ… Hugging Face Hubì— ëª¨ë¸ ì—…ë¡œë“œ
* âœ… ë¬¸ì¥ ìš”ì•½ ì™¸ì— ë‹¤ë¬¸ì„œ ìš”ì•½ í™•ì¥

ì´ ì¤‘ ì›í•˜ì‹œëŠ” í•­ëª©ì´ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”. ê·¸ì— ë§ëŠ” ì½”ë“œë„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
